The user wants to add a function to save the results of the analysis in the provided Python script. I will add a section for saving dataframes to CSV files, specifically for the "–¢–∞–±–ª–∏—Ü–∞ —Å—É–¥–æ–≤-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤" and the "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞" dataframes.

Here's how I'll modify the script:

1.  **Add a new header for download options.**
2.  **Add a `to_csv` function to convert dataframes to CSV and provide a download button.**
3.  **Apply this function to the `display_df` (–°—É–¥–∞-–∫–∞–Ω–¥–∏–¥–∞—Ç—ã).**
4.  **Apply this function to the `ship_incident_counts`, `ship_area_sum`, `spill_candidate_counts`, and `prime_suspects_df` dataframes from the "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞" section.**

<!-- end list -->

```python
import streamlit as st
import pandas as pd
import geopandas as gpd
import folium
from folium.plugins import HeatMap
import plotly.express as px
from streamlit_folium import st_folium
from datetime import timedelta

# --- 1. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –ó–∞–≥–æ–ª–æ–≤–æ–∫ ---
st.set_page_config(layout="wide", page_title="–ê–Ω–∞–ª–∏–∑ '–°—É–¥–Ω–æ-–ü—è—Ç–Ω–æ'")

st.title("üö¢ –ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–∏ '–°—É–¥–Ω–æ-–ü—è—Ç–Ω–æ' üíß")
st.write("""
–ó–∞–≥—Ä—É–∑–∏—Ç–µ GeoJSON —Å –ø–æ–ª–∏–≥–æ–Ω–∞–º–∏ —Ä–∞–∑–ª–∏–≤–æ–≤ –∏ CSV-—Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ AIS.
–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞–π–¥–µ—Ç —Å—É–¥–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—Ö–æ–¥–∏–ª–∏—Å—å –≤ –∑–æ–Ω–µ —Ä–∞–∑–ª–∏–≤–∞ –Ω–µ–∑–∞–¥–æ–ª–≥–æ –¥–æ –µ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è,
–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –∞–Ω–∞–ª–∏—Ç–∏–∫—É –ø–æ –∏–Ω—Ü–∏–¥–µ–Ω—Ç–∞–º.
""")

# --- 2. –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å —Å –∑–∞–≥—Ä—É–∑—á–∏–∫–∞–º–∏ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ ---
st.sidebar.header("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞")

spills_file = st.sidebar.file_uploader(
    "1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ GeoJSON —Å –ø–æ–ª–∏–≥–æ–æ–Ω–∞–º–∏ —Ä–∞–∑–ª–∏–≤–æ–≤",
    type=["geojson", "json"]
)
ais_file = st.sidebar.file_uploader(
    "2. –ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV —Å –¥–∞–Ω–Ω—ã–º–∏ AIS",
    type=["csv"]
)

time_window_hours = st.sidebar.slider(
    "3. –í—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ –ø–æ–∏—Å–∫–∞ (—á–∞—Å—ã –¥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è):",
    min_value=1, max_value=168, value=24, step=1,
    help="–ò—Å–∫–∞—Ç—å —Å—É–¥–∞, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –≤ –∑–æ–Ω–µ —Ä–∞–∑–ª–∏–≤–∞ –∑–∞ —É–∫–∞–∑–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å–æ–≤ –î–û –µ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è."
)

# --- 3. –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö ---

@st.cache_data
def load_spills_data(uploaded_file):
    st.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ GeoJSON —Å —Ä–∞–∑–ª–∏–≤–∞–º–∏...")
    try:
        gdf = gpd.read_file(uploaded_file)
    except Exception as e:
        st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å GeoJSON —Ñ–∞–π–ª. –û—à–∏–±–∫–∞: {e}")
        return None

    required_cols = ['slick_name', 'area_sys']
    if not all(col in gdf.columns for col in required_cols):
        missing = [col for col in required_cols if col not in gdf.columns]
        st.error(f"–í GeoJSON –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è: {', '.join(missing)}")
        return None

    gdf.rename(columns={'slick_name': 'spill_id', 'area_sys': 'area_sq_km'}, inplace=True)

    if 'date' in gdf.columns and 'time' in gdf.columns:
        st.success("–û–±–Ω–∞—Ä—É–∂–µ–Ω —Ñ–æ—Ä–º–∞—Ç —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ 'date' –∏ 'time'.")
        gdf['detection_date'] = pd.to_datetime(gdf['date'] + ' ' + gdf['time'], errors='coerce')
    else:
        st.success("–û–±–Ω–∞—Ä—É–∂–µ–Ω —Ñ–æ—Ä–º–∞—Ç —Å –¥–∞—Ç–æ–π –≤ ID –ø—è—Ç–Ω–∞. –ü–∞—Ä—Å–∏–Ω–≥ 'spill_id'...")
        gdf['detection_date'] = pd.to_datetime(gdf['spill_id'], format='%Y-%m-%d_%H:%M:%S', errors='coerce')

    if gdf['detection_date'].isnull().any():
        failed_count = gdf['detection_date'].isnull().sum()
        st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –¥–∞—Ç—É –≤ {failed_count} –∑–∞–ø–∏—Å—è—Ö –æ —Ä–∞–∑–ª–∏–≤–∞—Ö. –≠—Ç–∏ –∑–∞–ø–∏—Å–∏ –±—É–¥—É—Ç –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω—ã.")
        gdf.dropna(subset=['detection_date'], inplace=True)

    if gdf.empty:
        st.error("–ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –Ω–∏ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏ –æ —Ä–∞–∑–ª–∏–≤–∞—Ö —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –¥–∞—Ç–æ–π.")
        return None

    if gdf.crs is None:
        gdf.set_crs("EPSG:4326", inplace=True)
    else:
        gdf = gdf.to_crs("EPSG:4326")

    st.success("–î–∞–Ω–Ω—ã–µ –æ —Ä–∞–∑–ª–∏–≤–∞—Ö —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã.")
    return gdf

@st.cache_data
def load_ais_data(uploaded_file):
    st.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ CSV —Å –¥–∞–Ω–Ω—ã–º–∏ AIS...")
    try:
        # –£–∫–∞–∑—ã–≤–∞–µ–º low_memory=False –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ—à–∏–±–æ–∫ —Å —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–∞—Ö
        df = pd.read_csv(uploaded_file, low_memory=False)
    except Exception as e:
        st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å CSV —Ñ–∞–π–ª. –û—à–∏–±–∫–∞: {e}")
        return None

    required_cols = ['mmsi', 'latitude', 'longitude', 'BaseDateTime']
    if not all(col in df.columns for col in required_cols):
        missing = [col for col in required_cols if col not in df.columns]
        st.error(f"–í CSV —Ñ–∞–π–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(missing)}")
        return None

    df['timestamp'] = pd.to_datetime(df['BaseDateTime'], errors='coerce')
    df.dropna(subset=['timestamp', 'latitude', 'longitude'], inplace=True)

    gdf = gpd.GeoDataFrame(
        df,
        geometry=gpd.points_from_xy(df.longitude, df.latitude),
        crs="EPSG:4326"
    )

    st.success("–î–∞–Ω–Ω—ã–µ AIS —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")
    return gdf

def find_candidates(spills_gdf, vessels_gdf, time_window_hours):
    if spills_gdf is None or vessels_gdf is None:
        return gpd.GeoDataFrame()

    candidates = gpd.sjoin(vessels_gdf, spills_gdf, predicate='within')

    if candidates.empty:
        return gpd.GeoDataFrame()

    time_delta = timedelta(hours=time_window_hours)
    candidates = candidates[
        (candidates['timestamp'] <= candidates['detection_date']) &
        (candidates['timestamp'] >= candidates['detection_date'] - time_delta)
    ]

    return candidates

# --- Function to convert DataFrame to CSV for download ---
@st.cache_data
def convert_df_to_csv(df):
    # IMPORTANT: Cache the conversion to prevent computation on every rerun
    return df.to_csv(index=False).encode('utf-8')

# --- 4. –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è ---
if spills_file and ais_file:
    spills_gdf = load_spills_data(spills_file)
    vessels_gdf = load_ais_data(ais_file)

    if spills_gdf is None or vessels_gdf is None or spills_gdf.empty or vessels_gdf.empty:
        st.stop()

    candidates_df = find_candidates(spills_gdf, vessels_gdf, time_window_hours)

    st.header("–ö–∞—Ä—Ç–∞ —Ä–∞–∑–ª–∏–≤–æ–≤ –∏ —Å—É–¥–æ–≤-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")

    map_center = [spills_gdf.unary_union.centroid.y, spills_gdf.unary_union.centroid.x]
    m = folium.Map(location=map_center, zoom_start=8, tiles="CartoDB positron")

    # –°–ª–æ–π —Å –ø—è—Ç–Ω–∞–º–∏
    spills_fg = folium.FeatureGroup(name="–ü—è—Ç–Ω–∞ —Ä–∞–∑–ª–∏–≤–æ–≤").add_to(m)
    for _, row in spills_gdf.iterrows():
        folium.GeoJson(
            row['geometry'],
            style_function=lambda x: {'fillColor': '#B22222', 'color': 'black', 'weight': 1.5, 'fillOpacity': 0.6},
            tooltip=f"<b>–ü—è—Ç–Ω–æ:</b> {row.get('spill_id', 'N/A')}<br>"
                    f"<b>–í—Ä–µ–º—è:</b> {row['detection_date'].strftime('%Y-%m-%d %H:%M')}<br>"
                    f"<b>–ü–ª–æ—â–∞–¥—å:</b> {row.get('area_sq_km', 0):.2f} –∫–º¬≤"
        ).add_to(spills_fg)

    if not candidates_df.empty:
        candidate_vessels_fg = folium.FeatureGroup(name="–°—É–¥–∞-–∫–∞–Ω–¥–∏–¥–∞—Ç—ã").add_to(m)
        for _, row in candidates_df.iterrows():
            vessel_name = row.get('vessel_name', '–ò–º—è –Ω–µ —É–∫–∞–∑–∞–Ω–æ')
            folium.Marker(
                location=[row.geometry.y, row.geometry.x],
                tooltip=f"<b>–°—É–¥–Ω–æ:</b> {vessel_name} (MMSI: {row['mmsi']})<br>"
                        f"<b>–í—Ä–µ–º—è –ø—Ä–æ—Ö–æ–¥–∞:</b> {row['timestamp'].strftime('%Y-%m-%d %H:%M')}<br>"
                        f"<b>–í–Ω—É—Ç—Ä–∏ –ø—è—Ç–Ω–∞:</b> {row['spill_id']}",
                icon=folium.Icon(color='blue', icon='ship', prefix='fa')
            ).add_to(candidate_vessels_fg)

    folium.LayerControl().add_to(m)
    st_folium(m, width=1200, height=500)

    st.header(f"–¢–∞–±–ª–∏—Ü–∞ —Å—É–¥–æ–≤-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (–Ω–∞–π–¥–µ–Ω–æ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö {time_window_hours} —á–∞—Å–æ–≤)")

    if candidates_df.empty:
        st.info("–í –∑–∞–¥–∞–Ω–Ω–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–º –æ–∫–Ω–µ —Å—É–¥–∞-–∫–∞–Ω–¥–∏–¥–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
    else:
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, –µ—Å–ª–∏ –æ–¥–Ω–æ —Å—É–¥–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –ø–æ–ø–∞–ª–æ –≤ –æ–¥–Ω–æ –ø—è—Ç–Ω–æ
        report_df = candidates_df.drop_duplicates(subset=['spill_id', 'mmsi'])
        desired_cols = ['spill_id', 'mmsi', 'vessel_name', 'timestamp', 'detection_date', 'area_sq_km']
        existing_cols = [col for col in desired_cols if col in report_df.columns]
        display_df = report_df[existing_cols].copy()

        rename_dict = {
            'spill_id': 'ID –ü—è—Ç–Ω–∞',
            'mmsi': 'MMSI –°—É–¥–Ω–∞',
            'vessel_name': '–ù–∞–∑–≤–∞–Ω–∏–µ —Å—É–¥–Ω–∞',
            'timestamp': '–í—Ä–µ–º—è –ø—Ä–æ—Ö–æ–¥–∞ —Å—É–¥–Ω–∞',
            'detection_date': '–í—Ä–µ–º—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø—è—Ç–Ω–∞',
            'area_sq_km': '–ü–ª–æ—â–∞–¥—å –ø—è—Ç–Ω–∞, –∫–º¬≤'
        }
        display_df.rename(columns=rename_dict, inplace=True)
        st.dataframe(display_df.sort_values(by='–í—Ä–µ–º—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø—è—Ç–Ω–∞', ascending=False).reset_index(drop=True))

        # --- DOWNLOAD BUTTON FOR CANDIDATE VESSELS TABLE ---
        csv_display_df = convert_df_to_csv(display_df)
        st.download_button(
            label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É —Å—É–¥–æ–≤-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∫–∞–∫ CSV",
            data=csv_display_df,
            file_name='candidate_vessels_report.csv',
            mime='text/csv',
        )

        # --- –ù–û–í–´–ô –ë–õ–û–ö –° –†–ê–°–®–ò–†–ï–ù–ù–û–ô –ê–ù–ê–õ–ò–¢–ò–ö–û–ô ---
        st.markdown("---")
        st.header("–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞")

        # –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ - —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ä—ã "—Å—É–¥–Ω–æ-–ø—è—Ç–Ω–æ"
        unique_incidents = candidates_df.drop_duplicates(subset=['mmsi', 'spill_id'])

        tab1, tab2, tab3 = st.tabs(["üìä –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø–æ —Å—É–¥–∞–º", "üìç –ì–æ—Ä—è—á–∏–µ —Ç–æ—á–∫–∏ (Hotspots)", "üîç –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø–æ –∏–Ω—Ü–∏–¥–µ–Ω—Ç–∞–º"])

        with tab1:
            st.subheader("–ê–Ω—Ç–∏—Ä–µ–π—Ç–∏–Ω–≥ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –ø—è—Ç–µ–Ω")
            ship_incident_counts = unique_incidents.groupby('mmsi').size().reset_index(name='incident_count') \
                .sort_values('incident_count', ascending=False).reset_index(drop=True)
            if 'vessel_name' in unique_incidents.columns:
                ship_names = unique_incidents[['mmsi', 'vessel_name']].drop_duplicates()
                ship_incident_counts = pd.merge(ship_incident_counts, ship_names, on='mmsi', how='left')
            st.dataframe(ship_incident_counts)
            csv_ship_incident_counts = convert_df_to_csv(ship_incident_counts)
            st.download_button(
                label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å –∞–Ω—Ç–∏—Ä–µ–π—Ç–∏–Ω–≥ —Å—É–¥–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø—è—Ç–µ–Ω",
                data=csv_ship_incident_counts,
                file_name='ship_incident_counts.csv',
                mime='text/csv',
                key='ship_incident_counts_dl'
            )
            
            st.subheader("–ê–Ω—Ç–∏—Ä–µ–π—Ç–∏–Ω–≥ –ø–æ —Å—É–º–º–∞—Ä–Ω–æ–π –ø–ª–æ—â–∞–¥–∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –ø—è—Ç–µ–Ω (–∫–º¬≤)")
            ship_area_sum = unique_incidents.groupby('mmsi')['area_sq_km'].sum().reset_index(name='total_area_sq_km') \
                .sort_values('total_area_sq_km', ascending=False).reset_index(drop=True)
            if 'vessel_name' in unique_incidents.columns:
                ship_area_sum = pd.merge(ship_area_sum, ship_names, on='mmsi', how='left')
            st.dataframe(ship_area_sum)
            csv_ship_area_sum = convert_df_to_csv(ship_area_sum)
            st.download_button(
                label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å –∞–Ω—Ç–∏—Ä–µ–π—Ç–∏–Ω–≥ —Å—É–¥–æ–≤ –ø–æ –ø–ª–æ—â–∞–¥–∏ –ø—è—Ç–µ–Ω",
                data=csv_ship_area_sum,
                file_name='ship_area_sum.csv',
                mime='text/csv',
                key='ship_area_sum_dl'
            )

        with tab2:
            st.subheader("–ö–∞—Ä—Ç–∞ '–≥–æ—Ä—è—á–∏—Ö —Ç–æ—á–µ–∫' —Ä–∞–∑–ª–∏–≤–æ–≤")
            m_heatmap = folium.Map(location=map_center, zoom_start=8, tiles="CartoDB positron")
            heat_data = [[point.xy[1][0], point.xy[0][0], row['area_sq_km']] for index, row in spills_gdf.iterrows() for point in [row['geometry'].centroid]]
            HeatMap(heat_data, radius=15, blur=20, max_zoom=10).add_to(m_heatmap)
            st_folium(m_heatmap, width=1200, height=500)

        with tab3:
            st.subheader("–ü—è—Ç–Ω–∞ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å—É–¥–æ–≤-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
            spill_candidate_counts = candidates_df.groupby('spill_id')['mmsi'].nunique().reset_index(name='candidate_count') \
                .sort_values('candidate_count', ascending=False).reset_index(drop=True)
            st.dataframe(spill_candidate_counts)
            csv_spill_candidate_counts = convert_df_to_csv(spill_candidate_counts)
            st.download_button(
                label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å –ø—è—Ç–Ω–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å—É–¥–æ–≤-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤",
                data=csv_spill_candidate_counts,
                file_name='spill_candidate_counts.csv',
                mime='text/csv',
                key='spill_candidate_counts_dl'
            )

            st.subheader("–ì–ª–∞–≤–Ω—ã–µ –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã–µ (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –¥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è)")
            candidates_df['time_to_detection'] = candidates_df['detection_date'] - candidates_df['timestamp']
            prime_suspects_idx = candidates_df.groupby('spill_id')['time_to_detection'].idxmin()
            prime_suspects_df = candidates_df.loc[prime_suspects_idx]

            display_cols = ['spill_id', 'mmsi', 'vessel_name', 'time_to_detection', 'area_sq_km']
            existing_display_cols = [col for col in display_cols if col in prime_suspects_df.columns]
            st.dataframe(prime_suspects_df[existing_display_cols].sort_values('area_sq_km', ascending=False))
            csv_prime_suspects_df = convert_df_to_csv(prime_suspects_df[existing_display_cols])
            st.download_button(
                label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É –≥–ª–∞–≤–Ω—ã—Ö –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã—Ö",
                data=csv_prime_suspects_df,
                file_name='prime_suspects_report.csv',
                mime='text/csv',
                key='prime_suspects_dl'
            )

        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –±–ª–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ —Ç–∏–ø–∞–º —Å—É–¥–æ–≤
        if 'VesselType' in unique_incidents.columns:
            with st.expander("üö¢ –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º —Å—É–¥–æ–≤"):
                vessel_type_analysis = unique_incidents.groupby('VesselType').agg(
                    incident_count=('spill_id', 'count'),
                    total_area_sq_km=('area_sq_km', 'sum')
                ).sort_values('incident_count', ascending=False).reset_index()

                st.dataframe(vessel_type_analysis)
                csv_vessel_type_analysis = convert_df_to_csv(vessel_type_analysis)
                st.download_button(
                    label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å –∞–Ω–∞–ª–∏—Ç–∏–∫—É –ø–æ —Ç–∏–ø–∞–º —Å—É–¥–æ–≤",
                    data=csv_vessel_type_analysis,
                    file_name='vessel_type_analysis.csv',
                    mime='text/csv',
                    key='vessel_type_analysis_dl'
                )

                fig = px.pie(vessel_type_analysis, names='VesselType', values='incident_count',
                             title='–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–Ω—Ü–∏–¥–µ–Ω—Ç–æ–≤ –ø–æ —Ç–∏–ø–∞–º —Å—É–¥–æ–≤',
                             labels={'VesselType':'–¢–∏–ø —Å—É–¥–Ω–∞', 'incident_count':'–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–Ω—Ü–∏–¥–µ–Ω—Ç–æ–≤'})
                st.plotly_chart(fig)

else:
    st.info("‚¨ÖÔ∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –æ–±–∞ —Ñ–∞–π–ª–∞ –Ω–∞ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å –∞–Ω–∞–ª–∏–∑.")

```
